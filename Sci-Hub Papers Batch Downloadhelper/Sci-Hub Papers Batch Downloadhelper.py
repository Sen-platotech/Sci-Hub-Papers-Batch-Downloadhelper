import requests
from bs4 import BeautifulSoup
import os
import threading
import pandas as pd
import re
from queue import Queue
import time
import sys
import glob
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

# ================= é…ç½®åŒºåŸŸ =================

# æ·±åº¦ä¼ªè£…æµè§ˆå™¨è¯·æ±‚å¤´
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.9',
    'Connection': 'keep-alive'
}

# å¿½ç•¥ SSL è¯ä¹¦è­¦å‘Š
requests.packages.urllib3.disable_warnings()

# å®šä¹‰å¯èƒ½çš„åˆ—åå˜ä½“ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
DOI_COL_VARIANTS = ['doi', 'di', 'article_doi', 'doi_link', 'accession_number']
TITLE_COL_VARIANTS = ['title', 'article_title', 'ti', 'publication_title', 'article title']

# ================= è¾…åŠ©å‡½æ•° =================

def clean_filename(text, max_len=80):
    """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
    if not text:
        return "Unknown_Title"
    cleaned = re.sub(r'[\\/:*?"<>|]', '_', str(text))
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    return cleaned[:max_len]

def load_domains(file_path='domains.txt'):
    """ä»å¤–éƒ¨æ–‡ä»¶åŠ è½½åŸŸåï¼Œå¹¶è‡ªåŠ¨è¡¥å…¨ https://"""
    domains = []
    if not os.path.exists(file_path):
        print(f"[è­¦å‘Š] æ‰¾ä¸åˆ° {file_path}ï¼Œå°†ä½¿ç”¨å†…ç½®é»˜è®¤åŸŸåã€‚")
        return ["https://sci-hub.se/  ", "https://sci-hub.st/  ", "https://sci-hub.ru/  "]
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#'):
                # === æ ¸å¿ƒä¿®å¤ï¼šè‡ªåŠ¨æ·»åŠ  https:// å‰ç¼€ ===
                if not line.startswith("http://") and not line.startswith("https://"):
                    line = "https://" + line
                
                if not line.endswith('/'):
                    line += '/'
                domains.append(line)
    
    if not domains:
        print("[è­¦å‘Š] åŸŸåæ–‡ä»¶ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤åŸŸåã€‚")
        return ["https://sci-hub.se/  "]
    
    print(f"[ç³»ç»Ÿ] å·²åŠ è½½ {len(domains)} ä¸ªå¯ç”¨åŸŸå: {domains}")
    return domains

def find_column_name(df, variants):
    """åœ¨DataFrameä¸­æ¨¡ç³ŠæŸ¥æ‰¾åŒ¹é…çš„åˆ—å"""
    columns = df.columns
    for col in columns:
        if str(col).lower().strip() in variants:
            return col
    for col in columns:
        for v in variants:
            if v in str(col).lower():
                return col
    return None

def read_all_excel_files(folder_path):
    """è¯»å–æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰Excelæ–‡ä»¶å¹¶åˆå¹¶"""
    all_files = glob.glob(os.path.join(folder_path, "*.xls*"))
    combined_tasks = []
    
    print(f"[ç³»ç»Ÿ] æ­£åœ¨æ‰«æ '{folder_path}' ä¸‹çš„ Excel æ–‡ä»¶...")
    if not all_files:
        print(f"[é”™è¯¯] '{folder_path}' æ–‡ä»¶å¤¹æ˜¯ç©ºçš„æˆ–ä¸å­˜åœ¨Excelæ–‡ä»¶ã€‚")
        return []

    for file in all_files:
        try:
            df = pd.read_excel(file)
            doi_col = find_column_name(df, DOI_COL_VARIANTS)
            title_col = find_column_name(df, TITLE_COL_VARIANTS)
            
            if not doi_col:
                print(f"[è·³è¿‡] æ–‡ä»¶ '{os.path.basename(file)}' æœªæ‰¾åˆ° DOI åˆ—ã€‚")
                continue
            
            print(f"[è¯»å–] æ–‡ä»¶ '{os.path.basename(file)}' (DOIåˆ—: {doi_col}, æ ‡é¢˜åˆ—: {title_col})")
            
            for index, row in df.iterrows():
                doi = str(row[doi_col]).strip()
                if not doi or len(doi) < 5 or doi.lower() == 'nan':
                    continue
                
                title = "Unknown_Title"
                if title_col and pd.notna(row[title_col]):
                    title = str(row[title_col]).strip()
                
                combined_tasks.append((doi, title))
                
        except Exception as e:
            print(f"[é”™è¯¯] è¯»å–æ–‡ä»¶ {file} å¤±è´¥: {e}")
            
    unique_tasks = list(set(combined_tasks))
    print(f"[ç»Ÿè®¡] å…±æå–åˆ° {len(combined_tasks)} æ¡æ•°æ®ï¼Œå»é‡åå‰©ä½™ {len(unique_tasks)} ä¸ªä¸‹è½½ä»»åŠ¡ã€‚")
    return unique_tasks

# ================= æ ¸å¿ƒçº¿ç¨‹é€»è¾‘ =================

def download_worker(queue, domains, save_dir, logs, processing_list, stats, lock):
    """ä¸‹è½½å·¥ä½œçº¿ç¨‹"""
    
    session = requests.Session()
    session.headers.update(HEADERS)
    
    retries = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
    session.mount('https://', HTTPAdapter(max_retries=retries))
    session.mount('http://', HTTPAdapter(max_retries=retries))

    while not queue.empty():
        try:
            doi, title = queue.get(block=False)
        except:
            break

        safe_title = clean_filename(title)
        safe_doi = clean_filename(doi, 30)
        task_id = f"{safe_doi}"
        
        with lock:
            task_info = {'id': task_id, 'title': safe_title[:20], 'status': 'Connecting...'}
            processing_list.append(task_info)
        
        success = False
        target_path = os.path.join(save_dir, f"{safe_title}.pdf")
        
        if safe_title == "Unknown_Title":
            target_path = os.path.join(save_dir, f"{clean_filename(doi)}.pdf")

        if os.path.exists(target_path):
             with lock:
                stats['exists'] += 1
                stats['completed'] += 1
                logs['success'].append(f"{doi}\tå·²å­˜åœ¨è·³è¿‡")
                queue.task_done()
                if task_info in processing_list: processing_list.remove(task_info)
             continue

        last_error = ""

        for domain in domains:
            try:
                base_url = domain.rstrip("/")
                url = f"{base_url}/{doi}"
                
                # 1. è·å–è¯¦æƒ…é¡µ
                r = session.get(url, timeout=20, verify=False, allow_redirects=True)
                r.raise_for_status()
                
                if "captcha" in r.text.lower():
                    raise Exception("è¢«Sci-Hubæ‹¦æˆª(éªŒè¯ç )")

                soup = BeautifulSoup(r.text, "html.parser")
                
                # 2. è§£æPDFé“¾æ¥
                download_url = None
                if soup.iframe and soup.iframe.get("src"):
                    download_url = soup.iframe.get("src")
                elif soup.embed and soup.embed.get("src"):
                    download_url = soup.embed.get("src")
                elif soup.find("div", {"id": "buttons"}):
                     btn = soup.find("div", {"id": "buttons"}).find("a")
                     if btn: download_url = btn.get("onclick").split("'")[1]

                if not download_url:
                    if r.url.endswith(".pdf"):
                        download_url = r.url
                    else:
                        raise Exception("é¡µé¢æœªæ‰¾åˆ°PDFé“¾æ¥")
                
                # è¡¥å…¨ URL
                if download_url.startswith("//"):
                    download_url = "https:" + download_url
                elif download_url.startswith("/"):
                    download_url = base_url + download_url
                elif not download_url.startswith("http"):
                    # å¤„ç†ç½•è§çš„ç›¸å¯¹è·¯å¾„ä¸”æ²¡æœ‰æ–œæ çš„æƒ…å†µ
                    download_url = base_url + "/" + download_url

                with lock:
                     for item in processing_list:
                        if item['id'] == task_id: item['status'] = 'Downloading PDF...'

                # 3. ä¸‹è½½æ–‡ä»¶
                pdf_r = session.get(download_url, timeout=30, verify=False)
                
                if len(pdf_r.content) < 1000 or b"%PDF-" not in pdf_r.content[:20]:
                     raise Exception("ä¸‹è½½å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„PDFæ–‡ä»¶")

                with open(target_path, "wb") as f:
                    f.write(pdf_r.content)
                success = True
                break 
                    
            except Exception as e:
                last_error = str(e)[:50]
                continue

        with lock:
            if success:
                stats['success'] += 1
                logs['success'].append(f"{doi}\tä¸‹è½½æˆåŠŸ")
            else:
                stats['failed'] += 1
                logs['error'].append(f"{doi}\t{safe_title}\tå¤±è´¥åŸå› : {last_error}")
            
            stats['completed'] += 1
            if task_info in processing_list:
                processing_list.remove(task_info)
        
        queue.task_done()

# ================= è¿›åº¦æ˜¾ç¤ºçº¿ç¨‹ =================

def progress_display(total, processing_list, stats, lock, stop_event):
    """æ§åˆ¶å°è¿›åº¦åˆ·æ–°"""
    start_time = time.time()
    
    while not stop_event.is_set():
        elapsed = time.time() - start_time
        with lock:
            curr_completed = stats['completed']
            curr_success = stats['success']
            curr_failed = stats['failed']
            curr_exists = stats['exists']
            current_tasks = list(processing_list)
        
        speed = curr_completed / elapsed if elapsed > 0 else 0
        percent = (curr_completed / total) * 100 if total > 0 else 0
        
        os.system('cls' if os.name == 'nt' else 'clear')
        
        print("="*80)
        print(f" Sci-Hub æ‰¹é‡ä¸‹è½½å™¨ v3.1 (URLè‡ªåŠ¨ä¿®å¤ç‰ˆ) - æ­£åœ¨è¿è¡Œ")
        print("="*80)
        print(f" [æ€»ä½“è¿›åº¦]: {percent:5.1f}% | å·²å®Œæˆ: {curr_completed}/{total}")
        print(f" [è¯¦ç»†ç»Ÿè®¡]: âœ… æˆåŠŸ: {curr_success} | âŒ å¤±è´¥: {curr_failed} | ğŸ“‚ è·³è¿‡: {curr_exists}")
        print(f" [å¹³å‡é€Ÿåº¦]: {speed:.2f} ç¯‡/ç§’ | è€—æ—¶: {elapsed:.1f} ç§’")
        print("-" * 80)
        print(" [å½“å‰çº¿ç¨‹çŠ¶æ€]:")
        
        for idx, task in enumerate(current_tasks[:8]):
            print(f"   {idx+1}. [{task['status']}] {task['title']}...")
            
        print("-" * 80)
        
        if curr_completed >= total:
            break
            
        time.sleep(0.5)

# ================= ä¸»ç¨‹åºå…¥å£ =================

if __name__ == "__main__":
    INPUT_FOLDER = r"./excel_files"
    DOWNLOAD_FOLDER = r"./downloaded_pdfs"
    DOMAIN_FILE = "domains.txt"
    THREAD_COUNT = 5

    if not os.path.exists(INPUT_FOLDER):
        os.makedirs(INPUT_FOLDER)
        print(f"[æç¤º] å·²åˆ›å»º '{INPUT_FOLDER}'ï¼Œè¯·å°†Excelæ–‡ä»¶æ”¾å…¥å…¶ä¸­åé‡æ–°è¿è¡Œã€‚")
        sys.exit()
    if not os.path.exists(DOWNLOAD_FOLDER):
        os.makedirs(DOWNLOAD_FOLDER)

    sci_hub_domains = load_domains(DOMAIN_FILE)
    tasks = read_all_excel_files(INPUT_FOLDER)

    if not tasks:
        print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ä»»åŠ¡ï¼Œç¨‹åºé€€å‡ºã€‚")
        sys.exit()

    task_queue = Queue()
    for t in tasks:
        task_queue.put(t)

    total_tasks = len(tasks)
    lock = threading.Lock()
    
    stats = {'completed': 0, 'success': 0, 'failed': 0, 'exists': 0}
    logs = {'success': [], 'error': []}
    processing_list = []
    
    stop_event = threading.Event()

    ui_thread = threading.Thread(target=progress_display, args=(total_tasks, processing_list, stats, lock, stop_event))
    ui_thread.daemon = True
    ui_thread.start()

    workers = []
    for _ in range(THREAD_COUNT):
        t = threading.Thread(target=download_worker, args=(task_queue, sci_hub_domains, DOWNLOAD_FOLDER, logs, processing_list, stats, lock))
        t.daemon = True
        t.start()
        workers.append(t)

    task_queue.join()
    stop_event.set()
    time.sleep(1)

    with open("download_success.log", "w", encoding="utf-8") as f:
        f.writelines([line + "\n" for line in logs['success']])
    
    with open("download_error.log", "w", encoding="utf-8") as f:
        f.writelines([line + "\n" for line in logs['error']])

    print("\n\næ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæ¯•ï¼")